#!/usr/bin/env python
import argparse
import os, sys


import torch
import torch.nn as nn
import torch.nn.functional as F

torch.__version__

from utils.dna_loader import *
from utils.train_pytorch import *


# define Binary FocalLoss

## a simplified version without alpha - for speed
class BinaryFocalLoss(nn.Module):
    def __init__(self, gamma=2):
        super(BinaryFocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        # Compute binary cross entropy loss
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')

        # Compute the modulating factor
        p_t = torch.exp(-BCE_loss)
        modulating_factor = (1 - p_t)**self.gamma

        # Compute the focal loss
        focal_loss = modulating_factor * BCE_loss

        return focal_loss.mean()



# Get cpu, gpu or mps device for training.
device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using {device} device, version {torch.__version__}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description = "Train deep neural network...)", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("--train", nargs='+', help = "training data *_seq.bin generated by 'write_data_bin.py' ", required = True)
    parser.add_argument("--val", nargs='+', help = "validation data *_seq.bin generated by 'write_data_bin.py", required = True)
    parser.add_argument("--model", required = True, help = "model class (in a python script) to import")

    parser.add_argument("--out", default = "model_out_dir", help = "model output directory")
    parser.add_argument('--window_len', type=int, default = 200, help = 'window len, default 200bp')
    parser.add_argument('--n_center_windows', type=int, default = 1, help = 'n center windows: n windows the model will predict lables')
    parser.add_argument('--n_pad_windows', type=int, default = 25, help = 'n padding windows: n windows paded in both sides. For example: seq len 1000 bp has n_pad_windows = 2 (*200) - or 400bp each sides; seq len 10200 bp has n_pad_windows = 25 (*200) - or 5000bp each sides.')
    parser.add_argument('--batch_size', type=int, default = 256, help = 'batch size for training')
    parser.add_argument('--threads', type=int, default = 0, help = 'CPU cores for data pipeline loading')
    parser.add_argument('--lr', type=float, default = 0.0005, help = 'Learning rate')
    parser.add_argument('--decay', type=float, default = 1e-6, help = 'Weight decay')
    parser.add_argument("--loss", choices=["focal", "logit"], default="logit", help="Choose loss function: focal or logit (default: logit)")

    ### input args
    args = parser.parse_args()
    train_files = args.train
    val_files = args.val
    out = args.out
    batch_size = args.batch_size
    window_len = args.window_len
    n_center_windows = args.n_center_windows
    n_pad_windows = args.n_pad_windows
    num_threads = args.threads
    model_in = args.model
    #use_logit = args.logit_loss
    lr = args.lr
    decay = args.decay
    loss_fn = agr.loss

    ## import model
    model_impoter = f'exec("from {model_in} import *")'
    eval(model_impoter)
    print(f"loaded model class {model_in}")

    ## data loaders
    train_data = load_data_long_sequence(train_files, n_center_windows, n_pad_windows, eval_mode = False,\
        random_reverse_complement=True, batch_size=batch_size, num_workers = num_threads)

    val_data_fw = load_data_long_sequence(val_files, n_center_windows, n_pad_windows, eval_mode = True,\
        random_reverse_complement=False, batch_size=batch_size, num_workers = num_threads)

    val_data_rc = load_data_long_sequence(val_files, n_center_windows, n_pad_windows, eval_mode = True,\
        random_reverse_complement=True, batch_size=batch_size, num_workers = num_threads)


    n_out = get_output_dim(val_files, n_center_windows, n_pad_windows, window_len)

    model = Model(n_out)
    model.to(device)
    print("number of params:")
    print(count_parameters(model))

    # If more than one GPU is available, use DataParallel
    num_gpus = torch.cuda.device_count()
    print(f"Number of GPUs available: {num_gpus}")

    if num_gpus > 1:
        # Wrap the model with DataParallel
        model = nn.DataParallel(model)

    if loss_fn == 'logit':
        criterion = nn.BCEWithLogitsLoss()
    if loss_fn == 'focal':
        criterion = BinaryFocalLoss()
    else:
        raise 'Not valid loss function'    

    optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = decay)
    trainer = Trainer(model, criterion, optimizer, binary_AUC, num_epochs = 100, early_stoper = 5)
    trainer.fit(train_data, val_data_fw, val_data_rc, out)